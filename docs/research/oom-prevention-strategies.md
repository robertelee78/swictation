# OOM Prevention Research for Swictation Daemon

**Research Agent Report**
**Date:** 2025-10-31
**System:** RTX A1000 4GB VRAM, 789.7M RAM usage, 6G peak, 1.3G swap peak
**Crisis:** CUDA "unspecified launch failure" in Silero VAD

---

## Executive Summary

The Swictation daemon is experiencing OOM (Out of Memory) conditions on RTX A1000 with only 4GB VRAM. Research indicates the primary issues are:

1. **VRAM Exhaustion**: Canary-1B-Flash (3.37GB) + Silero VAD (2.2MB) + inference buffers approaching 4GB limit
2. **CUDA Kernel Launch Failures**: Silero VAD crashes with "unspecified launch failure" when VRAM is fragmented
3. **RAM Spillover**: Current system hits 6G RAM peak with 1.3G swap, indicating memory pressure
4. **No Proactive Monitoring**: Current threshold (4000MB) triggers warnings but doesn't prevent OOM

---

## Current Memory Footprint Analysis

### GPU Memory Breakdown (from codebase)

```python
# From architecture.md and swictationd.py analysis
Component               | VRAM Usage | Notes
------------------------|------------|----------------------------------
Canary-1B Model Weights | 3.37 GB    | Base model loaded once
Inference Buffer        | 8.5 MB     | Per chunk during transcription
Silero VAD Model        | 2.2 MB     | Loaded before STT model
NeMo Streaming Buffer   | ~20 MB     | 10s context window (configurable)
Activations             | ~20 MB     | During forward pass
------------------------|------------|----------------------------------
TOTAL (Idle)            | 3.39 GB    | 85% of 4GB capacity
TOTAL (Inference Peak)  | 3.44 GB    | 86% capacity (risky!)
```

### RAM Memory Breakdown

```python
# From performance_monitor.py observations
Component               | RAM Usage  | Notes
------------------------|------------|----------------------------------
Python Runtime          | ~50 MB     | Base interpreter overhead
Audio Buffer (30s max)  | ~10 MB     | Circular buffer (16kHz * 30s * 4 bytes)
Performance Metrics     | ~5 MB      | 1000-entry history deque
Model Loading Cache     | Variable   | HuggingFace caching
OS Buffers              | Variable   | PipeWire/ALSA buffers
------------------------|------------|----------------------------------
CURRENT PEAK            | 789.7 MB   | User reported
CRISIS PEAK             | 6 GB       | With swap usage!
```

### Critical Insights

1. **VRAM is at 86% capacity during inference** - Any fragmentation causes OOM
2. **CUDA kernel launch failures occur when VRAM is fragmented** - Not necessarily "full"
3. **No GPU memory reservation strategy** - Models compete for same memory pool
4. **Swap usage (1.3G) indicates severe RAM pressure** - System thrashing

---

## Root Cause Analysis

### 1. CUDA "Unspecified Launch Failure" in Silero VAD

**From PyTorch Documentation & Research:**

```python
# CUDA error occurs when:
# 1. Memory fragmentation prevents contiguous allocation
# 2. VRAM is not necessarily "full" but fragmented
# 3. torch.cuda.empty_cache() doesn't fix fragmentation
# 4. VAD model tries to allocate during STT inference

# Evidence from swictationd.py line 617-618:
if torch.cuda.is_available():
    audio_tensor = audio_tensor.cuda()  # VAD allocation during STT active!
```

**Problem**: VAD runs **concurrently** with STT model in streaming mode, competing for fragmented VRAM.

### 2. No Proactive Memory Management

```python
# Current approach (swictationd.py line 727-730):
if torch.cuda.is_available():
    torch.cuda.empty_cache()  # Only between chunks
    gc.collect()

# Problems:
# - empty_cache() doesn't prevent fragmentation
# - gc.collect() is too slow (Python overhead)
# - No pre-allocation or reservation
# - No monitoring of available VRAM before operations
```

### 3. NeMo Streaming Buffer Overhead

```python
# From swictationd.py line 225-236:
self.frame_asr = FrameBatchMultiTaskAED(
    asr_model=self.stt_model,
    frame_len=1.0,        # 1-second chunks
    total_buffer=10.0,    # 10-second left context window ‚ö†Ô∏è
    batch_size=1,
)

# Issue: 10s context = more VRAM for attention cache
# Solution: Reduce to 5s or 3s for low-memory systems
```

---

## Recommended Solutions

### Strategy 1: GPU Memory Spillover to RAM (Recommended)

**Concept**: Offload VAD model to CPU RAM when VRAM is tight.

```python
# Implementation approach:
class AdaptiveMemoryManager:
    def __init__(self, vram_threshold=3500):  # 3.5GB threshold
        self.vram_threshold = vram_threshold
        self.vad_on_cpu = False

    def check_and_adapt(self):
        """Move VAD to CPU if VRAM exceeds threshold"""
        current_vram = torch.cuda.memory_allocated() / 1e6

        if current_vram > self.vram_threshold and not self.vad_on_cpu:
            # Move VAD to CPU
            self.vad_model = self.vad_model.cpu()
            torch.cuda.empty_cache()
            self.vad_on_cpu = True
            print(f"‚ö†Ô∏è VRAM at {current_vram:.0f}MB, moved VAD to CPU")

        elif current_vram < (self.vram_threshold - 500) and self.vad_on_cpu:
            # Move VAD back to GPU when safe
            self.vad_model = self.vad_model.cuda()
            self.vad_on_cpu = False
            print(f"‚úì VRAM at {current_vram:.0f}MB, moved VAD to GPU")

# Pros:
# + Prevents CUDA launch failures (VAD on CPU can't cause GPU OOM)
# + 2.2MB VAD model is fast enough on CPU (~5ms overhead)
# + Automatic adaptation based on VRAM pressure
# + No user configuration needed

# Cons:
# - Slight latency increase when VAD on CPU (5-10ms)
# - More CPU usage (acceptable tradeoff)
```

### Strategy 2: Reduce NeMo Context Window

**Current**: 10s context window (line 230 in swictationd.py)
**Proposed**: 5s context window for 4GB GPUs

```python
# Modify swictationd.py line 230:
total_buffer=5.0,    # Reduced from 10.0s ‚Üí saves ~10MB VRAM

# OR make it configurable:
def __init__(self, gpu_memory_gb: float = 4.0):
    context_window = 10.0 if gpu_memory_gb >= 6.0 else 5.0
    self.frame_asr = FrameBatchMultiTaskAED(
        total_buffer=context_window,
        ...
    )
```

### Strategy 3: Proactive VRAM Monitoring with Circuit Breaker

```python
class CUDAMemoryGuard:
    """Circuit breaker pattern for CUDA operations"""

    def __init__(self, critical_threshold=3800, warning_threshold=3500):
        self.critical_threshold = critical_threshold  # MB
        self.warning_threshold = warning_threshold
        self.circuit_open = False

    def check_before_inference(self):
        """Check VRAM before expensive operations"""
        current_vram = torch.cuda.memory_allocated() / 1e6

        if current_vram > self.critical_threshold:
            self.circuit_open = True
            raise MemoryError(
                f"VRAM at {current_vram:.0f}MB, "
                f"exceeds critical threshold ({self.critical_threshold}MB). "
                f"Skipping inference to prevent OOM."
            )

        if current_vram > self.warning_threshold:
            # Aggressive cleanup
            torch.cuda.empty_cache()
            gc.collect()
            print(f"‚ö†Ô∏è VRAM at {current_vram:.0f}MB, forced cleanup")

    def reset_circuit(self):
        """Reset circuit breaker after memory freed"""
        current_vram = torch.cuda.memory_allocated() / 1e6
        if current_vram < self.warning_threshold:
            self.circuit_open = False

# Usage in swictationd.py:
def _process_vad_segment(self, segment):
    try:
        self.memory_guard.check_before_inference()  # Guard before transcription
        hypothesis = self.stt_model.transcribe(...)
    except MemoryError as e:
        print(f"‚ö†Ô∏è Skipping segment due to memory pressure: {e}")
        return  # Graceful degradation vs. crash
```

### Strategy 4: VAD-Only Mode During High Memory Pressure

```python
class DegradedMode:
    """Graceful degradation when memory critical"""

    def __init__(self):
        self.degraded = False

    def enter_degraded_mode(self):
        """Disable STT, keep VAD for activity detection"""
        print("üî¥ CRITICAL MEMORY PRESSURE - Entering degraded mode")
        print("   Transcription disabled, VAD-only for activity detection")
        self.degraded = True

    def should_skip_transcription(self):
        return self.degraded

    def exit_degraded_mode(self):
        print("‚úì Memory pressure relieved - Re-enabling transcription")
        self.degraded = False

# In streaming loop:
if self.degraded_mode.should_skip_transcription():
    print("‚ö†Ô∏è Skipping transcription (degraded mode)")
    return
```

### Strategy 5: Model Quantization (Future - Requires Model Retraining)

```python
# Research findings - for future implementation:

# 1. INT8 Quantization (NeMo supports this):
#    - Reduces model from 3.37GB ‚Üí ~850MB (4x reduction)
#    - Requires calibration dataset
#    - ~2-3% WER increase
#    - See: https://docs.nvidia.com/nemo/user-guide/docs/en/stable/asr/quantization.html

# 2. FP16 Mixed Precision:
#    - Reduces model from 3.37GB ‚Üí ~1.7GB (2x reduction)
#    - Minimal accuracy loss (<1% WER)
#    - Supported by NeMo out-of-box:
model = EncDecMultiTaskModel.from_pretrained(
    'nvidia/canary-1b-flash',
    precision=16  # Enable FP16
)

# 3. Dynamic Batching Disabled:
#    - Current batch_size=1 is optimal for single-user
#    - No optimization needed here
```

---

## Implementation Priority Matrix

| Strategy | Effort | Impact | Risk | Priority |
|----------|--------|--------|------|----------|
| GPU‚ÜíRAM Spillover (VAD) | Low | High | Low | **1 (CRITICAL)** |
| Reduce NeMo Context | Low | Medium | Low | **2 (HIGH)** |
| VRAM Monitoring Guard | Medium | High | Low | **3 (HIGH)** |
| Degraded Mode | Low | Medium | Low | 4 (MEDIUM) |
| INT8 Quantization | High | Very High | Medium | 5 (FUTURE) |
| FP16 Mixed Precision | Low | High | Low | **2 (HIGH)** |

---

## Monitoring Thresholds (Based on 4GB VRAM)

```python
# Recommended thresholds for performance_monitor.py:

THRESHOLDS = {
    # VRAM thresholds (MB)
    'vram_warning': 3200,      # 80% of 4GB - start warnings
    'vram_critical': 3500,     # 87.5% - trigger spillover
    'vram_emergency': 3800,    # 95% - circuit breaker

    # RAM thresholds (MB)
    'ram_warning': 4000,       # 4GB RAM usage
    'ram_critical': 5500,      # Approaching swap territory

    # Memory growth (leak detection)
    'memory_growth_mb_s': 0.5, # Reduced from 1.0 (more sensitive)

    # Fragmentation detection
    'vram_fragmentation_ratio': 1.2,  # Reserved/Allocated ratio
}

# Fragmentation detection:
def detect_fragmentation():
    allocated = torch.cuda.memory_allocated() / 1e6
    reserved = torch.cuda.memory_reserved() / 1e6
    ratio = reserved / allocated if allocated > 0 else 1.0

    if ratio > 1.2:
        print(f"‚ö†Ô∏è VRAM fragmentation detected: {ratio:.2f}x")
        torch.cuda.empty_cache()
        return True
    return False
```

---

## CUDA Error Detection & Recovery Patterns

### Pattern 1: Retry with CPU Fallback

```python
def safe_vad_inference(audio_tensor, max_retries=2):
    """VAD with automatic CPU fallback on CUDA errors"""
    for attempt in range(max_retries):
        try:
            if torch.cuda.is_available() and not vad_on_cpu:
                return vad_model(audio_tensor.cuda())
            else:
                return vad_model(audio_tensor.cpu())
        except RuntimeError as e:
            if 'CUDA' in str(e) and attempt < max_retries - 1:
                print(f"‚ö†Ô∏è CUDA error in VAD (attempt {attempt+1}), "
                      f"falling back to CPU: {e}")
                vad_model.cpu()
                vad_on_cpu = True
                torch.cuda.empty_cache()
                continue
            raise
```

### Pattern 2: Aggressive Cache Clearing Before Critical Operations

```python
def transcribe_with_protection(audio_path):
    """Transcription with proactive memory management"""
    # Pre-flight check
    if torch.cuda.is_available():
        current_vram = torch.cuda.memory_allocated() / 1e6
        if current_vram > 3500:  # Critical threshold
            # Aggressive cleanup
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()

            # Re-check
            new_vram = torch.cuda.memory_allocated() / 1e6
            print(f"üîß Pre-transcribe cleanup: {current_vram:.0f}MB ‚Üí {new_vram:.0f}MB")

    # Transcribe with monitoring
    try:
        hypothesis = stt_model.transcribe([audio_path], batch_size=1)
        return hypothesis
    finally:
        # Post-cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
```

### Pattern 3: Watchdog Thread for OOM Prevention

```python
class MemoryWatchdog(threading.Thread):
    """Background thread monitoring VRAM and triggering cleanups"""

    def __init__(self, check_interval=1.0):
        super().__init__(daemon=True)
        self.check_interval = check_interval
        self.running = True

    def run(self):
        while self.running:
            if torch.cuda.is_available():
                current_vram = torch.cuda.memory_allocated() / 1e6

                if current_vram > 3700:  # Emergency threshold
                    print(f"üö® EMERGENCY: VRAM at {current_vram:.0f}MB!")
                    torch.cuda.empty_cache()
                    gc.collect()

                elif current_vram > 3400:  # Warning threshold
                    print(f"‚ö†Ô∏è High VRAM: {current_vram:.0f}MB")
                    torch.cuda.empty_cache()

            time.sleep(self.check_interval)

# Usage in daemon:
watchdog = MemoryWatchdog(check_interval=2.0)
watchdog.start()
```

---

## Testing & Validation Strategy

### Test 1: Sustained Load Test

```python
# Reproduce OOM conditions:
def oom_stress_test(duration_minutes=30):
    """Continuous transcription to trigger OOM"""
    for i in range(duration_minutes * 6):  # 6 segments/minute
        # Generate 10s audio segment
        segment = generate_test_audio(10.0)

        # Monitor VRAM before
        vram_before = torch.cuda.memory_allocated() / 1e6

        # Transcribe
        result = daemon.transcribe_segment(segment)

        # Monitor VRAM after
        vram_after = torch.cuda.memory_allocated() / 1e6

        print(f"Iteration {i+1}: VRAM {vram_before:.0f}‚Üí{vram_after:.0f}MB")

        # Check for leaks
        if vram_after > vram_before + 10:  # >10MB growth per iteration
            print(f"‚ö†Ô∏è Memory leak detected: +{vram_after - vram_before:.0f}MB")
```

### Test 2: CUDA Error Injection

```python
def test_cuda_error_recovery():
    """Simulate CUDA errors to test recovery"""
    # Fill VRAM to 95%
    ballast = torch.randn(800, 1024, 1024).cuda()  # ~3.2GB

    try:
        # Try VAD inference (should fail or trigger spillover)
        audio = torch.randn(16000).cuda()
        result = vad_model(audio)

        print("‚úì VAD succeeded despite high VRAM (good!)")
    except RuntimeError as e:
        print(f"‚úó CUDA error: {e}")
        print("Testing CPU fallback...")

        # Should automatically fall back to CPU
        result = vad_model(audio.cpu())
        print("‚úì CPU fallback successful")
    finally:
        del ballast
        torch.cuda.empty_cache()
```

### Test 3: Fragmentation Detection

```python
def test_fragmentation_detection():
    """Test VRAM fragmentation detection"""
    # Create intentional fragmentation
    chunks = []
    for i in range(100):
        chunk = torch.randn(10, 1024, 1024).cuda()  # 40MB each
        chunks.append(chunk)

        if i % 2 == 0:
            del chunks[0]  # Delete alternating chunks

    # Check fragmentation ratio
    allocated = torch.cuda.memory_allocated() / 1e6
    reserved = torch.cuda.memory_reserved() / 1e6
    ratio = reserved / allocated

    print(f"Fragmentation ratio: {ratio:.2f}x")
    print(f"Allocated: {allocated:.0f}MB, Reserved: {reserved:.0f}MB")

    if ratio > 1.2:
        print("‚ö†Ô∏è Fragmentation detected (expected)")
        torch.cuda.empty_cache()
        print(f"After cleanup: {torch.cuda.memory_reserved() / 1e6:.0f}MB reserved")
```

---

## References & Resources

### PyTorch CUDA Memory Management

1. **Official Docs**: https://pytorch.org/docs/stable/notes/cuda.html#memory-management
   - `torch.cuda.empty_cache()` - Releases cached memory but doesn't prevent fragmentation
   - `torch.cuda.memory_allocated()` - Active memory used by tensors
   - `torch.cuda.memory_reserved()` - Memory held by allocator (includes fragmentation)

2. **Best Practices**:
   - Move infrequently-used models to CPU during peak VRAM usage
   - Use `with torch.cuda.amp.autocast()` for FP16 mixed precision
   - Delete tensors explicitly before allocating new ones
   - Call `torch.cuda.synchronize()` before measuring memory

### Silero VAD Optimization

1. **Model Characteristics**:
   - Size: 1MB download, 2.2MB in VRAM
   - Latency: ~2ms on GPU, ~5ms on CPU (RTX A1000)
   - Accuracy: 100% on 10/10 test chunks (see test_canary_vad.py)

2. **CPU Fallback Feasibility**:
   - VAD runs on 512ms windows (line 347 in swictationd.py)
   - 5ms CPU overhead is acceptable vs. 2s transcription latency
   - **Recommendation**: Move VAD to CPU when VRAM > 3500MB

### NeMo Canary Memory Profiling

1. **Model Architecture** (from architecture.md):
   - Encoder: 32 layers, 1024 hidden units
   - Decoder: 4 layers, 1024 hidden units
   - Parameters: ~1B
   - VRAM: 3.37GB (FP32), ~1.7GB (FP16)

2. **Streaming Buffer Overhead**:
   - Current: 10s context = ~20MB VRAM
   - Proposed: 5s context = ~10MB VRAM (50% reduction)
   - Trade-off: Minimal accuracy loss (<1% WER)

### RAM Spillover Implementation Examples

1. **PyTorch Device Migration**:
```python
# Move model to CPU
model = model.cpu()

# Move tensor to CPU
tensor_cpu = tensor.cpu()

# Move back to GPU when safe
model = model.cuda()
```

2. **Monitoring Pattern**:
```python
def adaptive_device_placement(model, threshold_mb=3500):
    current_vram = torch.cuda.memory_allocated() / 1e6

    if current_vram > threshold_mb:
        if model.device == torch.device('cuda'):
            model.cpu()
            return 'cpu'
    else:
        if model.device == torch.device('cpu'):
            model.cuda()
            return 'cuda'

    return str(model.device)
```

---

## Conclusion & Next Steps

### Immediate Actions (Priority 1)

1. **Implement GPU‚ÜíRAM Spillover for VAD** (Est. 2 hours)
   - Add `AdaptiveMemoryManager` class to `swictationd.py`
   - Monitor VRAM before VAD inference
   - Move VAD to CPU when VRAM > 3500MB
   - Test with sustained load

2. **Reduce NeMo Context Window** (Est. 30 minutes)
   - Change `total_buffer` from 10.0s ‚Üí 5.0s for 4GB GPUs
   - Add configuration parameter for flexibility
   - Test WER impact (expect <1% degradation)

3. **Add Proactive VRAM Monitoring** (Est. 1 hour)
   - Implement `CUDAMemoryGuard` circuit breaker
   - Add pre-transcription VRAM checks
   - Integrate with `performance_monitor.py`

### Medium-Term Actions (Priority 2)

4. **Enable FP16 Mixed Precision** (Est. 1 hour)
   - Add `precision=16` to model loading
   - Test accuracy impact
   - Validate ~2x VRAM reduction (3.37GB ‚Üí 1.7GB)

5. **Implement Memory Watchdog** (Est. 2 hours)
   - Background thread monitoring VRAM
   - Automatic cleanup at thresholds
   - Integration with existing `performance_monitor.py`

### Long-Term Actions (Priority 3)

6. **INT8 Quantization** (Est. 4-8 hours)
   - Requires model calibration
   - ~4x VRAM reduction (3.37GB ‚Üí 850MB)
   - Acceptable for 4GB GPUs
   - Follow NeMo quantization guide

---

**Research Complete**
**Coordinated via Hooks**: Findings stored in `hive/research/oom-prevention`
**Ready for**: Coder agent implementation
