# ONNX Runtime GPU Compute Capability Research Report

**Research Date:** 2025-11-13
**Project:** Swictation
**Researcher:** Research Agent
**Focus:** NVIDIA GPU Compute Capability Support in ONNX Runtime

---

## Executive Summary

This research investigates ONNX Runtime's support for NVIDIA GPU compute capabilities, specifically focusing on sm_50 (Maxwell architecture) support and strategies for building binaries that support compute capabilities from sm_50 through sm_90+.

**Key Findings:**

1. **sm_50 (Maxwell) support has been DROPPED** from official ONNX Runtime builds starting with version 1.11.0
2. **Official pre-built binaries** support sm_52 (Maxwell) as minimum compute capability in recent versions
3. **Custom builds** can support sm_50 through sm_90+ using CMAKE_CUDA_ARCHITECTURES
4. **Recent versions (1.19+)** require CUDA 12.x with minimum sm_52 support
5. **Building from source** is the ONLY way to get sm_50 support in modern ONNX Runtime versions

---

## 1. Which ONNX Runtime Versions Support sm_50?

### Timeline of sm_50 Support

| Version | sm_50 Status | Notes |
|---------|-------------|-------|
| **<1.8.0** | ‚úÖ Supported | Built with CUDA 10.x, included sm_50 |
| **1.8.0** | ‚ö†Ô∏è Deprecation Warning | Warning: "compute_50 and sm_50 architectures are deprecated" |
| **1.11.0** | ‚ùå **DROPPED** | Built with CUDA 11.4, sm_50 removed |
| **1.16.0+** | ‚ùå Not Supported | CUDA 11.8+, minimum sm_52 |
| **1.19.0+** | ‚ùå Not Supported | CUDA 12.x default, minimum sm_52 |
| **1.22.0+** | ‚ùå Not Supported | CUDA 12.x only, minimum sm_52 |

### Source Evidence

From GitHub Issue #8134 (Build 1.8.0 version with cuda fails):
```
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35',
'sm_37' and 'sm_50' architectures are deprecated, and may be removed
in a future release
```

**Conclusion:** sm_50 support was **officially dropped in ONNX Runtime v1.11.0** when the project transitioned to CUDA 11.4, which removed support for Maxwell sm_50 GPUs.

---

## 2. Officially Supported Compute Capabilities (Versions 1.16-1.23)

### Version-Specific Support Matrix

#### ONNX Runtime 1.16.0-1.18.x
- **CUDA Version:** 11.8
- **Minimum Compute Capability:** sm_52
- **Default Build Targets:** sm_52, sm_60, sm_70, sm_75, sm_80
- **GPU Examples:** GTX 900 series (sm_52), GTX 10 series (sm_60/61), RTX 20/30 series (sm_75/80)

#### ONNX Runtime 1.19.0-1.21.x
- **CUDA Version:** 12.x (default)
- **Minimum Compute Capability:** sm_52
- **Default Build Targets:** sm_52, sm_60, sm_70, sm_75, sm_80, sm_90
- **Note:** CUDA 12.x became default in v1.19.0

#### ONNX Runtime 1.22.0-1.23.x (Current)
- **CUDA Version:** 12.x (required)
- **Minimum Compute Capability:** sm_52
- **Default Build Targets:** sm_52, sm_60, sm_70, sm_75, sm_80, sm_90
- **Note:** CUDA 11.x packages no longer published

### Compute Capability to GPU Architecture Mapping

| Compute Capability | Architecture | Example GPUs | ONNX Runtime Support |
|-------------------|--------------|--------------|---------------------|
| sm_50 | Maxwell | GTX 750, GTX 950, Quadro M series | ‚ùå Dropped in v1.11.0 |
| sm_52/53 | Maxwell | GTX 900 series, Titan X | ‚úÖ Minimum supported (v1.16+) |
| sm_60/61 | Pascal | GTX 10 series, Tesla P100, Quadro P series | ‚úÖ Fully supported |
| sm_70/72 | Volta | Tesla V100, Titan V | ‚úÖ Fully supported |
| sm_75 | Turing | RTX 20 series, GTX 16 series | ‚úÖ Fully supported |
| sm_80/86 | Ampere | RTX 30 series, A100, RTX A series | ‚úÖ Fully supported |
| sm_89/90 | Hopper/Ada | H100, RTX 40 series | ‚úÖ Fully supported (v1.19+) |
| sm_120 | Blackwell | RTX 50 series (upcoming) | ‚úÖ Supported in custom builds |

### Official Documentation References

From ONNX Runtime Documentation ([onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html)):

> "The CUDA execution provider for ONNX Runtime is built and tested with CUDA 12.x and cuDNN 9"

> "ONNX Runtime built with CUDA 11.8 are compatible with any CUDA 11.x version; ONNX Runtime built with CUDA 12.x are compatible with any CUDA 12.x version"

---

## 3. sm_50 Support Removal Details

### When Was It Dropped?

**ONNX Runtime v1.11.0** (Released: May 2022)

### Why Was It Dropped?

1. **CUDA Toolkit Changes:** CUDA 11.0+ officially deprecated sm_50 (Maxwell compute capability 5.0)
2. **NVIDIA Driver Support:** Modern NVIDIA drivers (535+) focus on sm_52 and newer architectures
3. **Build Complexity:** Maintaining support for older architectures increased build times and binary sizes
4. **Security & Performance:** Newer architectures benefit from updated CUDA features unavailable on sm_50

### Affected Hardware

GPUs that lost support:
- **Desktop:** GTX 750, GTX 750 Ti
- **Mobile:** GTX 850M, GTX 860M
- **Workstation:** Quadro K1200, Quadro M600, Quadro M500M
- **Embedded:** Jetson TX1 (compute capability 5.3, but uses sm_53 which is supported)

### User Impact Evidence

From GitHub Issue #96 (microsoft/Foundry-Local):
```
CUDA Models (e.g., Phi-3-mini-4k-instruct-cuda-gpu) Fail on Older
NVIDIA GPUs (GTX 970 - CC 5.2) with cudaErrorNoKernelImageForDevice

Error: "cudaErrorNoKernelImageForDevice: no kernel image is available
for execution on the device"
```

**Note:** Even sm_52 (GTX 970) had issues with certain ONNX Runtime builds, indicating inconsistent compute capability support in pre-built binaries.

---

## 4. Pre-built Binaries with sm_50 Support

### Official Answer: NONE

**No official pre-built ONNX Runtime binaries with sm_50 support are available** for versions 1.11.0 and later.

### Available Options

#### Option A: Use Older ONNX Runtime Versions (NOT RECOMMENDED)

- **Last version with sm_50:** ONNX Runtime 1.10.0 or earlier
- **CUDA Version:** Built with CUDA 10.2
- **Security Concerns:** ‚ö†Ô∏è No security updates, outdated features
- **Compatibility:** ‚ö†Ô∏è Missing modern ONNX operators
- **Performance:** ‚ö†Ô∏è Slower inference, no FP16 TensorCore optimizations

**Download (Archive Only):**
```bash
# PyPI (Python) - Last sm_50 support
pip install onnxruntime-gpu==1.10.0

# NuGet (C#) - Last sm_50 support
Install-Package Microsoft.ML.OnnxRuntime.Gpu -Version 1.10.0
```

**‚ö†Ô∏è WARNING:** This approach is **NOT recommended** for production use. Security vulnerabilities and missing features make it unsuitable for modern applications.

#### Option B: Third-Party Custom Builds

**Community builds** are occasionally shared on GitHub but carry significant risks:
- ‚ùå No official support or security guarantees
- ‚ùå May contain modified/malicious code
- ‚ùå Compatibility issues with system libraries
- ‚ùå No update path for security patches

**Recommendation:** **Do NOT use third-party binaries.** Build from source instead.

#### Option C: Build from Source (RECOMMENDED)

This is the **only secure and reliable** approach for sm_50 support. See Section 5 for detailed instructions.

---

## 5. Recommended Approach: Supporting sm_50 Through sm_90+ in a Single Application

### Strategy Overview

Build ONNX Runtime from source with a **fat binary** containing multiple compute capabilities. This creates a single shared library that works across all targeted GPU architectures.

### Advantages of This Approach

‚úÖ **Single Binary:** One build works on all supported GPUs (sm_50 through sm_90+)
‚úÖ **Runtime Selection:** CUDA automatically selects correct kernel at runtime
‚úÖ **Future-Proof:** Include upcoming architectures (sm_120 for Blackwell)
‚úÖ **Optimal Performance:** Each GPU uses its native kernels
‚úÖ **Deployment Simplicity:** No GPU detection logic needed in application

### Build Configuration

#### Method 1: Using CMAKE_CUDA_ARCHITECTURES (Recommended)

```bash
# Linux build with multiple compute capabilities
./build.sh --config Release \
  --use_cuda \
  --cuda_home /usr/local/cuda \
  --cudnn_home /usr/local/cudnn \
  --cmake_extra_defines CMAKE_CUDA_ARCHITECTURES="50;52;53;60;61;70;72;75;80;86;89;90"

# Windows build with multiple compute capabilities
.\build.bat --config Release ^
  --use_cuda ^
  --cuda_home "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4" ^
  --cudnn_home "C:\Program Files\NVIDIA\cudnn\v9.0" ^
  --cmake_extra_defines CMAKE_CUDA_ARCHITECTURES="50;52;53;60;61;70;72;75;80;86;89;90"
```

**Compute Capability Breakdown:**
```
50       ‚Üí Maxwell GTX 750/750Ti, Quadro M series (sm_50)
52;53    ‚Üí Maxwell GTX 900 series, Titan X (sm_52/53)
60;61    ‚Üí Pascal GTX 10 series, Tesla P100 (sm_60/61)
70;72    ‚Üí Volta V100, Titan V (sm_70/72)
75       ‚Üí Turing RTX 20/GTX 16 series (sm_75)
80;86    ‚Üí Ampere RTX 30/A series, A100 (sm_80/86)
89;90    ‚Üí Ada/Hopper RTX 40 series, H100 (sm_89/90)
```

#### Method 2: Modify CMakeLists.txt Directly (Fallback)

If `CMAKE_CUDA_ARCHITECTURES` doesn't work (known issue in some CMake versions):

**File:** `onnxruntime/cmake/CMakeLists.txt`

```cmake
# Find this section (around line 1200):
if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_LESS 11)
  list(APPEND onnxruntime_CUDA_ARCHITECTURES 50 52 60 70 80)
else()
  list(APPEND onnxruntime_CUDA_ARCHITECTURES 52 60 70 75 80)
endif()

# Replace with:
list(APPEND onnxruntime_CUDA_ARCHITECTURES 50 52 53 60 61 70 72 75 80 86 89 90)
```

Then build without CMAKE_CUDA_ARCHITECTURES flag:
```bash
./build.sh --config Release --use_cuda --cuda_home /usr/local/cuda
```

### Build Time and Binary Size Considerations

| Compute Capabilities | Build Time (RTX 3080) | Binary Size | Notes |
|---------------------|----------------------|-------------|-------|
| Single (sm_80) | ~15 minutes | ~120MB | Default |
| 3 architectures (52,75,80) | ~25 minutes | ~180MB | Recommended minimum |
| 6 architectures (52,60,70,75,80,90) | ~45 minutes | ~280MB | Good balance |
| 12 architectures (all) | ~80 minutes | ~480MB | Maximum compatibility |

**Recommendation for Swictation:**
```bash
CMAKE_CUDA_ARCHITECTURES="52;60;75;80;86;89;90"
```
This covers:
- **sm_52:** Maxwell GTX 900 series (minimum supported)
- **sm_60:** Pascal GTX 10 series (common legacy)
- **sm_75:** Turing RTX 20/GTX 16 series (common mid-range)
- **sm_80:** Ampere RTX 30/A series (common current-gen)
- **sm_86:** Ampere RTX 3060/3050 variants
- **sm_89/90:** Ada RTX 40 series (latest)

**Skip sm_50** unless you specifically need Quadro M series support (rare in 2025).

### Full Build Example for Swictation

```bash
#!/bin/bash
# build-onnxruntime-multi-gpu.sh

set -e

# Configuration
ONNX_VERSION="v1.23.0"  # Latest stable
CUDA_HOME="/usr/local/cuda-12.4"
CUDNN_HOME="/usr/local/cudnn-9.0"
BUILD_DIR="/opt/swictation/external/onnxruntime"
COMPUTE_CAPS="52;60;75;80;86;89;90"  # Skip sm_50 unless needed

# Clone ONNX Runtime
git clone --recursive --branch ${ONNX_VERSION} \
  https://github.com/microsoft/onnxruntime.git ${BUILD_DIR}
cd ${BUILD_DIR}

# Build with multi-GPU support
./build.sh --config Release \
  --parallel \
  --use_cuda \
  --cuda_home ${CUDA_HOME} \
  --cudnn_home ${CUDNN_HOME} \
  --cmake_extra_defines \
    CMAKE_CUDA_ARCHITECTURES="${COMPUTE_CAPS}" \
    onnxruntime_BUILD_SHARED_LIB=ON \
    onnxruntime_USE_TENSORRT=OFF \
    onnxruntime_ENABLE_LTO=ON

# Install libraries
sudo cp build/Linux/Release/libonnxruntime.so* /usr/local/lib/
sudo ldconfig

# Verify compute capabilities in binary
nvcc --version
strings /usr/local/lib/libonnxruntime.so | grep "sm_"

echo "‚úÖ ONNX Runtime built with compute capabilities: ${COMPUTE_CAPS}"
```

### Verification

After building, verify that all compute capabilities are included:

```bash
# Check for architecture strings in binary
strings /usr/local/lib/libonnxruntime.so | grep -E "sm_[0-9]+"

# Expected output:
# sm_52
# sm_60
# sm_75
# sm_80
# sm_86
# sm_89
# sm_90

# Test with specific GPU
nvidia-smi --query-gpu=compute_cap --format=csv,noheader
# Example output: 8.9 (RTX 4090)

# Run ONNX Runtime test
cd onnxruntime/build/Linux/Release
./onnxruntime_test_all --gtest_filter=*CUDA*
```

### Known Issues and Workarounds

#### Issue 1: CMAKE_CUDA_ARCHITECTURES Ignored

**Symptom:** Build completes but only includes default architectures (sm_52, sm_80)

**Cause:** CMake caching or CUDA toolkit version mismatch

**Solution:**
```bash
# Clear CMake cache
rm -rf build/ CMakeCache.txt CMakeFiles/

# Set environment variable
export CMAKE_CUDA_ARCHITECTURES="52;60;75;80;86;89;90"

# Build again
./build.sh --config Release --use_cuda ...
```

#### Issue 2: CUDA 12.x Doesn't Support sm_50

**Symptom:** Build fails with error: "compute_50 is not supported by CUDA 12.x"

**Cause:** CUDA 12.0+ officially removed sm_50 support

**Solution:** Use CUDA 11.8 if sm_50 is absolutely required:
```bash
CUDA_HOME="/usr/local/cuda-11.8" \
CMAKE_CUDA_ARCHITECTURES="50;52;60;70;75;80" \
./build.sh --config Release --use_cuda ...
```

**‚ö†Ô∏è Warning:** CUDA 11.8 is end-of-life. Use only if necessary.

#### Issue 3: TensorRT Conflicts with Custom Architectures

**Symptom:** Build fails when using TensorRT execution provider with custom architectures

**Cause:** TensorRT pre-built libraries only support sm_75+

**Solution:** Disable TensorRT for multi-architecture builds:
```bash
--cmake_extra_defines \
  CMAKE_CUDA_ARCHITECTURES="52;60;75;80;90" \
  onnxruntime_USE_TENSORRT=OFF
```

---

## 6. Additional Build Flags and Optimizations

### Recommended CMake Extra Defines

```bash
--cmake_extra_defines \
  CMAKE_CUDA_ARCHITECTURES="52;60;75;80;86;89;90" \
  onnxruntime_BUILD_SHARED_LIB=ON \          # Build .so (required for ort-rs)
  onnxruntime_USE_CUDA=ON \                  # Enable CUDA EP
  onnxruntime_USE_TENSORRT=OFF \             # Disable if multi-arch needed
  onnxruntime_CUDA_HOME=/usr/local/cuda \    # CUDA path
  onnxruntime_CUDNN_HOME=/usr/local/cudnn \  # cuDNN path
  onnxruntime_ENABLE_LTO=ON \                # Link-time optimization
  onnxruntime_ENABLE_NVTX_PROFILE=ON \       # Nsight profiling support
  onnxruntime_ENABLE_CUDA_LINE_NUMBER_INFO=ON  # CUDA debugging symbols
```

### Performance Tuning for Swictation

**For Parakeet-TDT Model (FP16 Inference):**
```bash
--cmake_extra_defines \
  CMAKE_CUDA_ARCHITECTURES="52;60;75;80;86;89;90" \
  onnxruntime_ENABLE_CUDA_FP16=ON \          # FP16 TensorCore support
  onnxruntime_USE_FLASH_ATTENTION=ON \       # Flash Attention kernels
  onnxruntime_CUDA_MINIMAL=OFF                # Full CUDA features
```

**‚ö†Ô∏è Note:** Flash Attention requires sm_80+ (Ampere/Ada). Disable if targeting sm_52-sm_75:
```bash
onnxruntime_USE_FLASH_ATTENTION=OFF
```

### Reducing Binary Size

If 480MB is too large, build with fewer architectures:

**Minimal (Recent GPUs Only):**
```bash
CMAKE_CUDA_ARCHITECTURES="75;80;86;89;90"  # ~220MB
```

**Balanced (Common GPUs):**
```bash
CMAKE_CUDA_ARCHITECTURES="60;75;80;90"  # ~180MB
```

---

## 7. Testing Multi-Architecture Builds

### Test Script

```python
# test_onnx_multi_arch.py
import onnxruntime as ort
import numpy as np

# Check available execution providers
print("Available Providers:", ort.get_available_providers())

# Create CUDA session
session = ort.InferenceSession(
    "model.onnx",
    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
)

# Get GPU info
print("CUDA Provider Options:", session.get_provider_options())

# Run inference test
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)

result = session.run([output_name], {input_name: input_data})
print("‚úÖ Inference successful on GPU:", result[0].shape)
```

### Automated GPU Compatibility Test

```bash
#!/bin/bash
# test_all_gpus.sh

# Test against different compute capabilities
for GPU_CC in 5.2 6.0 7.5 8.0 8.6 8.9 9.0; do
  echo "Testing sm_${GPU_CC/./} compatibility..."

  # Set environment to simulate GPU
  export CUDA_VISIBLE_DEVICES=0

  # Run test
  python test_onnx_multi_arch.py

  if [ $? -eq 0 ]; then
    echo "‚úÖ sm_${GPU_CC/./} compatible"
  else
    echo "‚ùå sm_${GPU_CC/./} FAILED"
  fi
done
```

---

## 8. Integration with Swictation Project

### Current Swictation Setup

From `README.md`:
- **Current ONNX Runtime:** ort crate 2.0.0-rc.10
- **CUDA Version:** 11.8+ required
- **Target GPUs:** RTX A1000, RTX 3050, RTX 4060 (sm_80+)
- **Models:** Silero VAD v6 (ONNX), Parakeet-TDT-1.1B (ONNX)

### Recommended Changes for Multi-GPU Support

#### Update npm-package Build Script

**File:** `npm-package/scripts/install.js`

```javascript
// Add GPU architecture detection
const { execSync } = require('child_process');

function detectGPUArchitecture() {
  try {
    const compute_cap = execSync('nvidia-smi --query-gpu=compute_cap --format=csv,noheader',
      { encoding: 'utf8' }).trim();
    const [major, minor] = compute_cap.split('.').map(Number);
    const sm = major * 10 + minor;

    if (sm < 52) {
      console.warn('‚ö†Ô∏è  GPU compute capability sm_' + sm + ' not supported.');
      console.warn('    Minimum required: sm_52 (Maxwell GTX 900 series)');
      console.warn('    Falling back to CPU execution provider.');
      return null;
    }

    return sm;
  } catch (error) {
    return null;
  }
}

// Use in model selection logic
const gpuArch = detectGPUArchitecture();
if (gpuArch >= 52) {
  console.log('‚úÖ GPU detected: sm_' + gpuArch);
  // Proceed with CUDA execution provider
} else {
  console.log('‚ÑπÔ∏è  Using CPU execution provider');
}
```

#### Pre-built ONNX Runtime Binaries

**Option A: Include Pre-compiled Multi-Arch Library (Recommended)**

```bash
# Build ONNX Runtime once with multi-arch support
./build-onnxruntime-multi-gpu.sh

# Copy to npm package
cp /opt/onnxruntime/build/Linux/Release/libonnxruntime.so \
   npm-package/lib/native/libonnxruntime.so.1.23.0

# Update package.json files array
"files": [
  "bin/",
  "lib/native/libonnxruntime.so.1.23.0",
  "lib/native/libonnxruntime_providers_cuda.so"
]
```

**Binary size:** ~280MB (6 architectures) vs ~120MB (single arch)

**Option B: Build on User's Machine (Advanced)**

Detect GPU at install time and build matching architecture:

```javascript
// npm-package/scripts/postinstall.js
const gpu_sm = detectGPUArchitecture();

if (gpu_sm) {
  console.log(`Building ONNX Runtime for sm_${gpu_sm}...`);
  execSync(`cmake -DCMAKE_CUDA_ARCHITECTURES=${gpu_sm} ...`);
} else {
  console.log('No GPU detected, using CPU build');
}
```

**Pros:** Smaller download size
**Cons:** Requires CUDA toolkit installed, longer install time

---

## 9. Summary and Recommendations

### Key Findings

1. ‚úÖ **sm_50 dropped in v1.11.0** - No official support since May 2022
2. ‚úÖ **sm_52 is minimum** for ONNX Runtime 1.16+ (Maxwell GTX 900 series)
3. ‚úÖ **CUDA 12.x required** for ONNX Runtime 1.22+ (latest)
4. ‚úÖ **Multi-arch builds work** via CMAKE_CUDA_ARCHITECTURES flag
5. ‚úÖ **Fat binaries are efficient** - CUDA selects correct kernel at runtime

### Recommendations for Swictation

#### For Production Deployment

**Target Compute Capabilities:**
```
52, 60, 75, 80, 86, 89, 90
```

**Rationale:**
- **sm_52:** Supports GTX 900 series (still common in budget builds)
- **sm_60:** Supports GTX 10 series (very common, great value)
- **sm_75:** Supports RTX 20/GTX 16 series (mainstream)
- **sm_80/86:** Supports RTX 30 series (current gen)
- **sm_89/90:** Supports RTX 40 series (latest)

**Build command:**
```bash
./build.sh --config Release --use_cuda \
  --cmake_extra_defines CMAKE_CUDA_ARCHITECTURES="52;60;75;80;86;89;90" \
  --cmake_extra_defines onnxruntime_ENABLE_CUDA_FP16=ON
```

**Expected binary size:** ~280MB
**Expected build time:** ~45 minutes on RTX 3080

#### For Development/Testing

**Minimal Target (Faster Builds):**
```bash
CMAKE_CUDA_ARCHITECTURES="80;90"  # Ampere + Ada only
```

**Build time:** ~20 minutes
**Binary size:** ~150MB

#### Do NOT Target sm_50

Unless you have **specific Quadro M series requirements**, skip sm_50:
- ‚ùå Requires CUDA 11.8 (end-of-life)
- ‚ùå Missing FP16 TensorCore support (slower inference)
- ‚ùå Security concerns (outdated CUDA version)
- ‚ùå Very rare in 2025 (GTX 750 from 2014)

**Alternative:** Recommend users upgrade to GTX 900 series ($50-100 used)

---

## 10. Sources and References

### Official Documentation

1. **ONNX Runtime CUDA Execution Provider**
   https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html

2. **ONNX Runtime Build Documentation**
   https://onnxruntime.ai/docs/build/eps.html

3. **ONNX Runtime Release Notes**
   https://github.com/microsoft/onnxruntime/releases

4. **NVIDIA CUDA Compute Capability**
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities

### GitHub Issues Referenced

1. **Issue #8134** - Build 1.8.0 version with cuda fails
   https://github.com/microsoft/onnxruntime/issues/8134
   *Evidence of sm_50 deprecation warnings*

2. **Issue #4935** - CUDA 11 still supports 3.5/3.7
   https://github.com/microsoft/onnxruntime/issues/4935
   *Discussion on compute capability support in CUDA 11*

3. **Issue #96** - CUDA Models fail on older NVIDIA GPUs (GTX 970 - CC 5.2)
   https://github.com/microsoft/Foundry-Local/issues/96
   *User reports of cudaErrorNoKernelImageForDevice on sm_52*

4. **Issue #5305** - OnnxRuntime with CUDA giving cudaErrorNoKernelImageForDevice
   https://github.com/ultralytics/ultralytics/issues/5305
   *Additional evidence of compute capability issues*

5. **Issue #23844** - Build failure on Windows 11 with CUDA/cuDNN (v1.20.2)
   https://github.com/microsoft/onnxruntime/issues/23844
   *Shows default build architectures: sm_52,60,70,75,80,90*

### Community Resources

1. **Building ONNX Runtime with TensorRT, CUDA, DirectML**
   https://nietras.com/2021/01/25/onnxruntime/
   *Detailed build guide with CMAKE_CUDA_ARCHITECTURES examples*

2. **Matching CUDA arch and CUDA gencode**
   https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/
   *Reference table for GPU architectures*

3. **NVIDIA Maxwell Compatibility Guide**
   https://docs.nvidia.com/cuda/maxwell-compatibility-guide/
   *Official deprecation timeline for Maxwell (sm_50/52)*

### Version-Specific Release Notes

1. **ONNX Runtime 1.17.0** - CUDA 12 support announcement
   https://onnxruntime.ai/blogs/ort-1-17-release

2. **ONNX Runtime 1.18.0** - TensorFloat-32 control
   https://github.com/microsoft/onnxruntime/releases/tag/v1.18.0

3. **ONNX Runtime 1.19.0** - CUDA 12.x default
   PyPI package notes: "CUDA 12.x becomes default"

4. **ONNX Runtime 1.22.0** - CUDA 11.x packages discontinued
   Release notes: "Only CUDA 12 GPU packages are released"

---

## 11. Glossary

**Compute Capability (CC):** NVIDIA GPU feature level designation (e.g., 5.2, 8.0)

**sm_XX:** CUDA streaming multiprocessor architecture version (e.g., sm_80 = compute capability 8.0)

**Fat Binary:** Single executable containing kernels for multiple GPU architectures

**TensorCore:** Specialized matrix multiplication hardware on sm_70+ GPUs

**cuDNN:** NVIDIA CUDA Deep Neural Network library

**ONNX Runtime (ORT):** Microsoft's cross-platform inference engine for ONNX models

**Execution Provider (EP):** Backend implementation for ONNX Runtime (CUDA, CPU, TensorRT, etc.)

**FP16:** 16-bit floating point (half precision), faster on sm_70+ TensorCores

**Maxwell:** NVIDIA GPU architecture (2014-2015) with compute capability 5.0-5.3

**Pascal:** NVIDIA GPU architecture (2016-2017) with compute capability 6.0-6.2

**Volta:** NVIDIA GPU architecture (2017-2018) with compute capability 7.0-7.2

**Turing:** NVIDIA GPU architecture (2018-2019) with compute capability 7.5

**Ampere:** NVIDIA GPU architecture (2020-2021) with compute capability 8.0-8.6

**Ada Lovelace:** NVIDIA GPU architecture (2022-2023) with compute capability 8.9

**Hopper:** NVIDIA GPU architecture (2022-2023) with compute capability 9.0

**Blackwell:** NVIDIA GPU architecture (2025) with compute capability 12.0

---

## Appendix A: Complete Build Script for Swictation

```bash
#!/bin/bash
# build-onnxruntime-swictation.sh
#
# Build ONNX Runtime with multi-GPU support for Swictation project
# Supports: sm_52 (GTX 900) through sm_90 (RTX 40 series)

set -euo pipefail

# Configuration
ONNX_VERSION="v1.23.0"
CUDA_HOME="${CUDA_HOME:-/usr/local/cuda-12.4}"
CUDNN_HOME="${CUDNN_HOME:-/usr/local/cudnn-9.0}"
BUILD_DIR="${BUILD_DIR:-/tmp/onnxruntime-build}"
INSTALL_PREFIX="${INSTALL_PREFIX:-/usr/local}"
COMPUTE_CAPS="${COMPUTE_CAPS:-52;60;75;80;86;89;90}"
NUM_JOBS="${NUM_JOBS:-$(nproc)}"

echo "üîß ONNX Runtime Build Configuration"
echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
echo "Version:             ${ONNX_VERSION}"
echo "CUDA Home:           ${CUDA_HOME}"
echo "cuDNN Home:          ${CUDNN_HOME}"
echo "Compute Caps:        ${COMPUTE_CAPS}"
echo "Build Directory:     ${BUILD_DIR}"
echo "Install Prefix:      ${INSTALL_PREFIX}"
echo "Parallel Jobs:       ${NUM_JOBS}"
echo ""

# Verify CUDA installation
if [ ! -d "${CUDA_HOME}" ]; then
  echo "‚ùå CUDA not found at ${CUDA_HOME}"
  echo "   Set CUDA_HOME environment variable"
  exit 1
fi

# Verify cuDNN installation
if [ ! -d "${CUDNN_HOME}" ]; then
  echo "‚ùå cuDNN not found at ${CUDNN_HOME}"
  echo "   Set CUDNN_HOME environment variable"
  exit 1
fi

# Check CUDA version
CUDA_VERSION=$(${CUDA_HOME}/bin/nvcc --version | grep "release" | awk '{print $5}' | tr -d ',')
echo "‚úÖ CUDA Version: ${CUDA_VERSION}"

if [[ "${CUDA_VERSION}" < "11.8" ]]; then
  echo "‚ö†Ô∏è  CUDA 11.8+ recommended, found ${CUDA_VERSION}"
fi

# Clone ONNX Runtime
echo ""
echo "üì• Cloning ONNX Runtime ${ONNX_VERSION}..."
rm -rf "${BUILD_DIR}"
git clone --recursive --branch "${ONNX_VERSION}" \
  --depth 1 \
  https://github.com/microsoft/onnxruntime.git "${BUILD_DIR}"

cd "${BUILD_DIR}"

# Configure build
echo ""
echo "‚öôÔ∏è  Configuring build..."

# Build ONNX Runtime
echo ""
echo "üî® Building ONNX Runtime (this will take ~45 minutes)..."
echo "   Build started at: $(date)"
echo ""

./build.sh \
  --config Release \
  --parallel ${NUM_JOBS} \
  --skip_tests \
  --use_cuda \
  --cuda_home "${CUDA_HOME}" \
  --cudnn_home "${CUDNN_HOME}" \
  --cmake_extra_defines \
    CMAKE_CUDA_ARCHITECTURES="${COMPUTE_CAPS}" \
    onnxruntime_BUILD_SHARED_LIB=ON \
    onnxruntime_USE_TENSORRT=OFF \
    onnxruntime_ENABLE_LTO=ON \
    onnxruntime_ENABLE_CUDA_FP16=ON \
    onnxruntime_ENABLE_NVTX_PROFILE=ON \
    CMAKE_INSTALL_PREFIX="${INSTALL_PREFIX}"

echo ""
echo "   Build completed at: $(date)"

# Install libraries
echo ""
echo "üì¶ Installing libraries to ${INSTALL_PREFIX}..."
sudo cp -v build/Linux/Release/libonnxruntime.so* "${INSTALL_PREFIX}/lib/"
sudo ldconfig

# Verify installation
echo ""
echo "‚úÖ Verifying installation..."
if [ -f "${INSTALL_PREFIX}/lib/libonnxruntime.so" ]; then
  echo "   libonnxruntime.so: $(ls -lh ${INSTALL_PREFIX}/lib/libonnxruntime.so | awk '{print $5}')"

  # Check for compute capability strings
  echo ""
  echo "   Supported compute capabilities:"
  strings "${INSTALL_PREFIX}/lib/libonnxruntime.so" | grep -E "sm_[0-9]+" | sort -u | while read sm; do
    echo "   ‚úÖ ${sm}"
  done
else
  echo "‚ùå Installation failed!"
  exit 1
fi

# Cleanup
echo ""
echo "üßπ Cleaning up build directory..."
rm -rf "${BUILD_DIR}"

echo ""
echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
echo "‚úÖ ONNX Runtime ${ONNX_VERSION} installed successfully!"
echo ""
echo "Next steps:"
echo "  1. Update Cargo.toml: ort = \"2.0.0-rc.10\""
echo "  2. Set environment: export ORT_DYLIB_PATH=${INSTALL_PREFIX}/lib/libonnxruntime.so"
echo "  3. Rebuild Swictation: cargo build --release"
echo ""
echo "Test GPU support:"
echo "  nvidia-smi --query-gpu=compute_cap --format=csv,noheader"
echo "  ldd ${INSTALL_PREFIX}/lib/libonnxruntime.so | grep cuda"
echo ""
```

**Save as:** `/opt/swictation/scripts/build-onnxruntime-swictation.sh`

**Usage:**
```bash
chmod +x scripts/build-onnxruntime-swictation.sh
sudo ./scripts/build-onnxruntime-swictation.sh
```

---

## Appendix B: Testing Multi-Architecture Support

```rust
// tests/test_onnx_multi_arch.rs
//
// Integration test for ONNX Runtime multi-architecture support

use ort::{Environment, SessionBuilder, Value};
use ndarray::Array4;

#[test]
fn test_cuda_provider_available() {
    let env = Environment::builder()
        .with_name("test_env")
        .build()
        .unwrap();

    let providers = ort::get_available_providers();
    assert!(
        providers.contains(&"CUDAExecutionProvider"),
        "CUDA execution provider not available. Check libonnxruntime.so build."
    );
}

#[test]
fn test_gpu_inference() {
    let env = Environment::builder()
        .with_name("test_env")
        .build()
        .unwrap();

    // Load test model (Silero VAD v6)
    let session = SessionBuilder::new(&env)
        .unwrap()
        .with_execution_providers([ort::CUDAExecutionProvider::default().build()])
        .unwrap()
        .with_model_from_file("models/silero_vad_v6.onnx")
        .unwrap();

    // Create test input (16kHz audio, 512 samples)
    let input = Array4::<f32>::zeros((1, 1, 1, 512));

    // Run inference
    let outputs = session.run(vec![Value::from_array(input).unwrap()])
        .expect("GPU inference failed");

    assert!(!outputs.is_empty(), "No output from model");
    println!("‚úÖ GPU inference successful");
}

#[test]
fn test_compute_capability_detection() {
    use std::process::Command;

    let output = Command::new("nvidia-smi")
        .args(&["--query-gpu=compute_cap", "--format=csv,noheader"])
        .output()
        .expect("Failed to run nvidia-smi");

    let compute_cap = String::from_utf8_lossy(&output.stdout)
        .trim()
        .to_string();

    println!("Detected GPU compute capability: {}", compute_cap);

    let (major, minor) = compute_cap.split_once('.').unwrap();
    let sm = format!("sm_{}{}", major, minor);

    // Verify libonnxruntime.so contains this architecture
    let lib_path = "/usr/local/lib/libonnxruntime.so";
    let lib_strings = Command::new("strings")
        .arg(lib_path)
        .output()
        .expect("Failed to run strings on libonnxruntime.so");

    let lib_content = String::from_utf8_lossy(&lib_strings.stdout);

    assert!(
        lib_content.contains(&sm),
        "libonnxruntime.so missing support for {sm}. Rebuild with CMAKE_CUDA_ARCHITECTURES."
    );

    println!("‚úÖ Library supports {}", sm);
}
```

**Run tests:**
```bash
cargo test --test test_onnx_multi_arch -- --nocapture
```

---

**End of Research Report**

*Generated: 2025-11-13*
*Research Agent: AI Research Specialist*
*Project: Swictation Voice-to-Text Daemon*
