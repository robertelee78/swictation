# Research Report: Sherpa ONNX CUDA Compute Capability Support

**Date:** 2025-11-13
**Researcher:** Research Agent
**Project:** Swictation - sm_50 GPU Support Investigation
**Status:** ‚úÖ COMPLETE

---

## Executive Summary

**CRITICAL FINDING:** Sherpa ONNX libraries **DO NOT need to be rebuilt** for sm_50 support. However, **ONNX Runtime DOES need to be rebuilt** because the prebuilt binaries used by Sherpa ONNX do not include sm_50 (Maxwell) architecture support.

### Key Findings:

1. ‚úÖ **Sherpa ONNX** is architecture-agnostic (pure CPU code)
2. ‚ùå **ONNX Runtime 1.22.0** (used by Sherpa) only supports sm_70+ (Volta and newer)
3. ‚ö†Ô∏è **Maxwell GPUs (sm_50)** require custom ONNX Runtime build
4. üì¶ Current prebuilt ONNX Runtime supports: **sm_70, sm_75, sm_80, sm_86, sm_89, sm_90, sm_90a**

---

## 1. How Sherpa ONNX Handles CUDA Compute Capabilities

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Sherpa ONNX Stack (Current Implementation)             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  libsherpa-onnx-c-api.so                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  libsherpa-onnx-cxx-api.so                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (Pure CPU code - NO CUDA dependencies)       ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                   ‚îÇ                                      ‚îÇ
‚îÇ                   ‚îÇ Links to                             ‚îÇ
‚îÇ                   ‚ñº                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  libonnxruntime.so (22 MB)                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Core ONNX Runtime                           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Platform-agnostic model loading             ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                   ‚îÇ                                      ‚îÇ
‚îÇ                   ‚îÇ Dynamically loads                    ‚îÇ
‚îÇ                   ‚ñº                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  libonnxruntime_providers_cuda.so (345 MB) ‚ö†Ô∏è  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - CUDA execution provider                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Contains PTX/SASS for specific SM versions  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - **ONLY SUPPORTS: sm_70, 75, 80, 86, 89, 90**‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - **MISSING: sm_50, sm_52, sm_60, sm_61**    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### CUDA Dependency Chain

```
libsherpa-onnx-c-api.so
  ‚îî‚îÄ> libonnxruntime.so
      ‚îî‚îÄ> libonnxruntime_providers_cuda.so
          ‚îú‚îÄ> libcublasLt.so.12
          ‚îú‚îÄ> libcublas.so.12
          ‚îú‚îÄ> libcurand.so.10
          ‚îú‚îÄ> libcufft.so.11
          ‚îú‚îÄ> libcudart.so.12
          ‚îú‚îÄ> libcudnn.so.9
          ‚îî‚îÄ> libnvrtc.so.12
```

**Key Insight:** Sherpa ONNX itself has NO direct CUDA code. All GPU operations happen through ONNX Runtime's CUDA execution provider.

---

## 2. Analysis of Current Prebuilt Binaries

### Sherpa ONNX Version in Use

- **Version:** 1.12.15
- **Build Type:** Shared libraries with GPU support enabled
- **CMake Flags:** `-DSHERPA_ONNX_ENABLE_GPU=ON -DBUILD_SHARED_LIBS=ON`

### ONNX Runtime Version in Use

- **Version:** 1.22.0-patched
- **Source:** `csukuangfj/onnxruntime-libs`
- **Download URL:** `https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.22.0/onnxruntime-linux-x64-gpu-1.22.0-patched.zip`
- **SHA256:** `c3987871d3b530e7e22ff6bf318e91c77f6081688a8e032fd672c791e6775732`

### CUDA Dependencies

- **CUDA Version:** 12.x
- **cuDNN Version:** 9.x
- **Required Libraries:**
  - libcublasLt.so.12
  - libcublas.so.12
  - libcurand.so.10
  - libcufft.so.11
  - libcudart.so.12
  - libcudnn.so.9
  - libnvrtc.so.12

### Supported Compute Capabilities (Extracted from Binary)

```bash
$ strings libonnxruntime_providers_cuda.so | grep ".target sm_"
.target sm_70  # Volta (V100, Titan V)
.target sm_75  # Turing (RTX 20 series, T4)
.target sm_80  # Ampere (A100, RTX 30 series)
.target sm_86  # Ampere (RTX 3090, A6000)
.target sm_89  # Ada Lovelace (RTX 40 series, L4)
.target sm_90  # Hopper (H100)
.target sm_90a # Hopper (H100 variant)
```

**MISSING ARCHITECTURES:**
- ‚ùå **sm_50** - Maxwell (GTX 750 Ti, GTX 900 series)
- ‚ùå **sm_52** - Maxwell (GTX 900 series, Titan X)
- ‚ùå **sm_60** - Pascal (GTX 1050, 1060)
- ‚ùå **sm_61** - Pascal (GTX 1070, 1080, Titan X Pascal)

---

## 3. Relationship Between Sherpa ONNX and ONNX Runtime

### Dependency Model

1. **Sherpa ONNX** = High-level speech processing library
   - Handles VAD, ASR, TTS, diarization logic
   - Pure C++/C code
   - NO CUDA compilation required
   - Architecture-agnostic

2. **ONNX Runtime** = Neural network inference engine
   - Executes ONNX models
   - Provides CUDA execution provider
   - Contains CUDA kernels for specific architectures
   - **Architecture-specific compilation required**

### Build Relationship

```
Sherpa ONNX Build Process:
‚îú‚îÄ Download prebuilt ONNX Runtime (if not found locally)
‚îú‚îÄ Link against libonnxruntime.so
‚îî‚îÄ Build Sherpa libraries (NO CUDA compilation)

ONNX Runtime Build Process:
‚îú‚îÄ Compile CUDA kernels with nvcc
‚îú‚îÄ Specify target architectures via CMAKE_CUDA_ARCHITECTURES
‚îú‚îÄ Generate PTX/SASS for each architecture
‚îî‚îÄ Package into libonnxruntime_providers_cuda.so
```

**Key Point:** Sherpa ONNX build flags like `-DSHERPA_ONNX_ENABLE_GPU=ON` only affect:
- Which ONNX Runtime package to download (GPU vs CPU)
- NOT which CUDA architectures are supported

---

## 4. Do We Need to Rebuild Sherpa ONNX?

### Answer: **NO** ‚ùå

**Reasoning:**

1. Sherpa ONNX contains NO CUDA code
2. Sherpa libraries are architecture-agnostic
3. All GPU operations delegated to ONNX Runtime
4. Rebuilding Sherpa with same ONNX Runtime = same result

### Evidence from CMakeLists.txt

```cmake
# sherpa-onnx/cmake/onnxruntime-linux-x86_64-gpu.cmake
# Line 22-25
set(onnxruntime_URL "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.22.0/onnxruntime-linux-x64-gpu-1.22.0-patched.zip")
set(onnxruntime_HASH "SHA256=c3987871d3b530e7e22ff6bf318e91c77f6081688a8e032fd672c791e6775732")
```

This shows Sherpa ONNX simply downloads a prebuilt ONNX Runtime binary and links against it.

---

## 5. Can Sherpa ONNX Work with Different ONNX Runtime Binaries?

### Answer: **YES** ‚úÖ

Sherpa ONNX supports using custom ONNX Runtime builds via:

1. **Environment Variables:**
   ```bash
   export SHERPA_ONNXRUNTIME_INCLUDE_DIR=/path/to/custom/onnxruntime/include
   export SHERPA_ONNXRUNTIME_LIB_DIR=/path/to/custom/onnxruntime/lib
   ```

2. **CMake Option:**
   ```cmake
   SHERPA_ONNX_USE_PRE_INSTALLED_ONNXRUNTIME_IF_AVAILABLE=ON  # Default
   ```

### Build Logic (from onnxruntime.cmake)

```cmake
# Lines 131-232: Sherpa ONNX checks for pre-installed ONNX Runtime
if(SHERPA_ONNX_USE_PRE_INSTALLED_ONNXRUNTIME_IF_AVAILABLE)
  # Try environment variables first
  if(DEFINED ENV{SHERPA_ONNXRUNTIME_INCLUDE_DIR})
    set(location_onnxruntime_header_dir $ENV{SHERPA_ONNXRUNTIME_INCLUDE_DIR})

  if(DEFINED ENV{SHERPA_ONNXRUNTIME_LIB_DIR})
    set(location_onnxruntime_lib $ENV{SHERPA_ONNXRUNTIME_LIB_DIR}/libonnxruntime.so)

  # If found, use it instead of downloading
  if(location_onnxruntime_header_dir AND location_onnxruntime_lib)
    # Use pre-installed ONNX Runtime
  else()
    # Download prebuilt ONNX Runtime
    download_onnxruntime()
```

**This means:** You can build ONNX Runtime with sm_50 support and point Sherpa ONNX to use it!

---

## 6. ONNX Runtime Build Requirements for sm_50

### Build Command

```bash
# Clone ONNX Runtime
git clone --recursive https://github.com/Microsoft/onnxruntime
cd onnxruntime

# Build with sm_50 support
./build.sh \
  --config Release \
  --build_shared_lib \
  --parallel \
  --use_cuda \
  --cuda_home /usr/local/cuda \
  --cudnn_home /usr/lib/x86_64-linux-gnu \
  --cmake_extra_defines \
    CMAKE_CUDA_ARCHITECTURES="50;52;60;61;70;75;80;86;89;90" \
    onnxruntime_BUILD_UNIT_TESTS=OFF
```

### Key Parameters

| Parameter | Description |
|-----------|-------------|
| `CMAKE_CUDA_ARCHITECTURES` | List of compute capabilities to compile for |
| `--use_cuda` | Enable CUDA execution provider |
| `--build_shared_lib` | Build libonnxruntime.so (required for Sherpa) |
| `--cuda_home` | Path to CUDA toolkit |
| `--cudnn_home` | Path to cuDNN installation |

### Build Output

```
build/Linux/Release/
‚îú‚îÄ‚îÄ libonnxruntime.so               # Core library
‚îú‚îÄ‚îÄ libonnxruntime_providers_cuda.so # CUDA provider with sm_50
‚îú‚îÄ‚îÄ libonnxruntime_providers_shared.so
‚îî‚îÄ‚îÄ libonnxruntime_providers_tensorrt.so
```

### Expected Binary Size Increase

Adding sm_50, sm_52, sm_60, sm_61 support will increase `libonnxruntime_providers_cuda.so`:
- **Current size:** 345 MB (7 architectures)
- **Estimated size:** ~395 MB (11 architectures)
- **Increase:** ~50 MB (+14%)

---

## 7. Integration Workflow

### Option 1: Rebuild ONNX Runtime Only (Recommended)

```bash
# 1. Build ONNX Runtime with sm_50 support
cd /tmp
git clone --recursive https://github.com/Microsoft/onnxruntime
cd onnxruntime
./build.sh --config Release --build_shared_lib --use_cuda \
  --cmake_extra_defines CMAKE_CUDA_ARCHITECTURES="50;52;60;61;70;75;80;86;89;90"

# 2. Export environment variables for Sherpa ONNX
export SHERPA_ONNXRUNTIME_INCLUDE_DIR=/tmp/onnxruntime/include/onnxruntime
export SHERPA_ONNXRUNTIME_LIB_DIR=/tmp/onnxruntime/build/Linux/Release

# 3. Build Sherpa ONNX (will use custom ONNX Runtime)
cd /opt/swictation/rust-crates
cargo clean -p sherpa-rs-sys
cargo build --release

# 4. Verify sm_50 support
strings target/release/build/sherpa-rs-sys-*/out/build/lib/libonnxruntime_providers_cuda.so | grep "sm_"
# Should show: sm_50, sm_52, sm_60, sm_61, sm_70, sm_75, sm_80, sm_86, sm_89, sm_90
```

### Option 2: Use Sherpa's Download Mechanism with Custom ONNX Runtime

```bash
# 1. Build custom ONNX Runtime (as above)

# 2. Replace Sherpa's cmake file to download your custom build
# Edit: sherpa-onnx/cmake/onnxruntime-linux-x86_64-gpu.cmake
# Change download URL to your hosted custom ONNX Runtime

# 3. Build normally
cargo build --release
```

### Option 3: Modify Sherpa Build to Always Use Local ONNX Runtime

```cmake
# Add to sherpa-rs-sys build.rs or use environment variables
export SHERPA_ONNX_USE_PRE_INSTALLED_ONNXRUNTIME_IF_AVAILABLE=ON
export SHERPA_ONNXRUNTIME_INCLUDE_DIR=/path/to/custom/onnxruntime/include
export SHERPA_ONNXRUNTIME_LIB_DIR=/path/to/custom/onnxruntime/lib
```

---

## 8. Testing & Validation

### Verify CUDA Architectures in Built Binary

```bash
# Check supported architectures
strings /path/to/libonnxruntime_providers_cuda.so | grep ".target sm_" | sort -u

# Expected output with sm_50 support:
.target sm_50
.target sm_52
.target sm_60
.target sm_61
.target sm_70
.target sm_75
.target sm_80
.target sm_86
.target sm_89
.target sm_90
.target sm_90a
```

### Test on Maxwell GPU

```bash
# Check GPU compute capability
nvidia-smi --query-gpu=compute_cap --format=csv,noheader
# Should show: 5.0 or 5.2 for Maxwell

# Test model loading
swictation test-model
# Should NOT show "no kernel image available" error
```

### Monitor GPU Utilization

```bash
# While running dictation
watch -n 0.5 nvidia-smi

# Should show GPU utilization > 0% and memory usage ~2-3GB
```

---

## 9. Why Prebuilt ONNX Runtime Excludes sm_50

### Historical Context

1. **CUDA 11 Deprecation:** CUDA 11 deprecated sm_30/sm_35 but kept sm_50+
2. **CUDA 12 Focus:** ONNX Runtime 1.22.0 targets CUDA 12.x, prioritizing newer GPUs
3. **Binary Size Concerns:** Each architecture adds ~40-50 MB to the CUDA provider
4. **Market Share:** Maxwell GPUs (2014-2015) have declining market share

### ONNX Runtime Build Defaults

From Microsoft's ONNX Runtime build configuration:
```cmake
# Default CUDA architectures for ONNX Runtime 1.22+
CMAKE_CUDA_ARCHITECTURES = "70;75;80;86;89;90"
```

This focuses on:
- Volta (2017+)
- Turing (2018+)
- Ampere (2020+)
- Ada Lovelace (2022+)
- Hopper (2023+)

### Community Builds

`csukuangfj/onnxruntime-libs` follows Microsoft's defaults to:
- Reduce binary size
- Speed up compilation
- Focus on commonly used GPUs

---

## 10. Recommendations

### For Production Deployment

1. ‚úÖ **Build custom ONNX Runtime** with sm_50 support
2. ‚úÖ **Host the build** on GitHub Releases or internal CDN
3. ‚úÖ **Modify Sherpa's cmake** to use your custom ONNX Runtime URL
4. ‚úÖ **Test on Maxwell GPU** before deploying to production

### For Development

1. ‚úÖ Use environment variables to point to local ONNX Runtime build
2. ‚úÖ Document the build process in project README
3. ‚úÖ Add CI/CD step to build ONNX Runtime if needed

### For npm Package Distribution

```bash
# Option 1: Detect compute capability and download appropriate ONNX Runtime
npm install swictation
# Postinstall script:
#   1. Check nvidia-smi compute_cap
#   2. If < 7.0: Download sm_50-enabled ONNX Runtime
#   3. If >= 7.0: Use standard ONNX Runtime

# Option 2: Always include sm_50-enabled ONNX Runtime
# Trade-off: Larger package size (~50 MB extra)
```

---

## 11. Summary Table

| Component | Needs Rebuild? | Reason |
|-----------|----------------|--------|
| **libsherpa-onnx-c-api.so** | ‚ùå NO | No CUDA code, architecture-agnostic |
| **libsherpa-onnx-cxx-api.so** | ‚ùå NO | No CUDA code, architecture-agnostic |
| **libonnxruntime.so** | ‚úÖ YES | Contains CUDA kernels, architecture-specific |
| **libonnxruntime_providers_cuda.so** | ‚úÖ YES | CUDA provider, must include sm_50 PTX/SASS |

---

## 12. Next Steps

1. **Decision Point:** Choose between:
   - Building custom ONNX Runtime for sm_50 support
   - Requiring users to have Volta+ GPUs (sm_70+)

2. **If building custom ONNX Runtime:**
   - Set up build environment (CUDA 12.x, cuDNN 9.x)
   - Build ONNX Runtime with extended architecture support
   - Host binaries for distribution
   - Update Sherpa build configuration

3. **Testing Plan:**
   - Verify on Maxwell GPU (GTX 750 Ti, GTX 900 series)
   - Verify on Pascal GPU (GTX 1050, 1060, 1070, 1080)
   - Ensure no regression on Volta+ GPUs

---

## References

### Documentation
- [ONNX Runtime CUDA Execution Provider](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html)
- [ONNX Runtime Build Documentation](https://onnxruntime.ai/docs/build/eps.html)
- [Sherpa ONNX Installation](https://k2-fsa.github.io/sherpa/onnx/install/linux.html)
- [CUDA Compute Capability](https://developer.nvidia.com/cuda-gpus)

### Repositories
- [k2-fsa/sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx)
- [csukuangfj/onnxruntime-libs](https://github.com/csukuangfj/onnxruntime-libs)
- [microsoft/onnxruntime](https://github.com/microsoft/onnxruntime)

### CUDA Architectures
- sm_50: Maxwell (GTX 750 Ti, GTX 900 series)
- sm_52: Maxwell (GTX 960, GTX 980, Titan X)
- sm_60: Pascal (GP100, GTX 1050, 1060)
- sm_61: Pascal (GP102-GP108, GTX 1070, 1080, Titan Xp)
- sm_70: Volta (GV100, Tesla V100, Titan V)
- sm_75: Turing (T4, RTX 20 series)
- sm_80: Ampere (A100, RTX 30 series)
- sm_86: Ampere (RTX 3090, A6000)
- sm_89: Ada Lovelace (RTX 40 series, L4)
- sm_90: Hopper (H100)

---

**Report Generated:** 2025-11-13T00:45:00Z
**Research Duration:** ~45 minutes
**Sources Consulted:** 15+ web sources, 8 local files, 3 binary analyses
