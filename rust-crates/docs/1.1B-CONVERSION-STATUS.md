# 1.1B Parakeet-TDT Model Conversion Status

**Date**: November 9, 2025
**Priority**: HIGHEST (1.1B is primary target model, not 110M)

## Current Status: ‚è≥ In Progress

### Phase 1: Conversion Toolkit ‚úÖ COMPLETE

Created proper conversion pipeline based on official k2-fsa/sherpa-onnx scripts:

**Files Created**:
- `/opt/swictation/models/convert_1.1b.py` - NeMo export script
- `/opt/swictation/models/convert-1.1b.sh` - Execution wrapper
- `/var/tmp/nemo_export/` - Isolated Python 3.12 venv (installing...)

**Key Features**:
- Uses NeMo's native export API (same as sherpa-onnx official scripts)
- Properly splits `decoder_joint` into separate `decoder.onnx` + `joiner.onnx`
- Exports both fp32 (for GPU) and int8 (for CPU) versions
- Creates proper `tokens.txt` from vocabulary
- Adds required metadata for sherpa-rs compatibility

### Phase 2: Environment Setup ‚è≥ IN PROGRESS

**Status**: Installing NeMo toolkit in isolated venv

```bash
# Background installation running:
cd /var/tmp && source nemo_export/bin/activate
pip install "nemo_toolkit[asr]" "numpy<2" onnx==1.17.0 onnxruntime==1.17.1
```

**Why venv?**: System has Python 3.13 but NeMo needs 3.12. Venv avoids ONNX version conflicts with existing sherpa-onnx installation.

**Time estimate**: 5-10 minutes (large dependency tree)

### Phase 3: Model Conversion üìã NEXT

Once venv ready, run conversion:

```bash
/opt/swictation/models/convert-1.1b.sh
```

**What it does**:
1. Downloads `nvidia/parakeet-tdt-1.1b` from HuggingFace (~1.2GB .nemo file)
2. Loads with NeMo ASR API
3. Exports encoder, decoder, joiner separately to ONNX
4. Creates tokens.txt from vocabulary
5. Quantizes to int8 for CPU option
6. Adds metadata

**Output**: `/opt/swictation/models/sherpa-onnx-nemo-parakeet-tdt-1.1b-converted/`

### Phase 4: Testing & GPU Benchmarking üìã PENDING

Once converted, test with sherpa-rs:

```rust
// Test fp32 with GPU
let mut recognizer = Recognizer::new(
    "/opt/swictation/models/sherpa-onnx-nemo-parakeet-tdt-1.1b-converted",
    true  // GPU
)?;

// Warmup
let _ = recognizer.recognize_file(warmup_sample)?;

// Benchmark inference time
let result = recognizer.recognize_file(audio_path)?;
```

**Expected Performance** (based on 110M fp32 results):
- CPU int8: ~300-400ms (higher accuracy than 0.6B)
- GPU fp32: ~20-50ms after warmup (estimated 5-10x speedup)
- Quality: Best available transcription accuracy

---

## Problem Being Solved

### Original Issue

The existing 1.1B model has **wrong file structure**:

```
‚ùå sherpa-onnx-nemo-parakeet-tdt-1.1b/
   ‚îú‚îÄ‚îÄ encoder-model.onnx          ‚úÖ
   ‚îú‚îÄ‚îÄ decoder_joint-model.onnx    ‚ùå COMBINED (wrong!)
   ‚îî‚îÄ‚îÄ (no tokens.txt)             ‚ùå MISSING
```

sherpa-rs requires **separate** files:

```
‚úÖ sherpa-onnx-nemo-parakeet-tdt-1.1b-converted/
   ‚îú‚îÄ‚îÄ encoder.onnx     ‚úÖ
   ‚îú‚îÄ‚îÄ decoder.onnx     ‚úÖ SPLIT
   ‚îú‚îÄ‚îÄ joiner.onnx      ‚úÖ SPLIT
   ‚îî‚îÄ‚îÄ tokens.txt       ‚úÖ
```

### Why NeMo Export Solves This

NeMo's `.export()` methods create separate ONNX files:

```python
asr_model.encoder.export("encoder.onnx")  # Acoustic features
asr_model.decoder.export("decoder.onnx")  # Label context
asr_model.joint.export("joiner.onnx")     # Fusion network
```

Raw NeMo checkpoint exports `decoder_joint` combined (for their runtime).
sherpa-onnx needs them split (for RNNT transducer architecture).

---

## Technical Details

### Based On Official Scripts

Our conversion is adapted from:
```
k2-fsa/sherpa-onnx/scripts/nemo/parakeet-tdt-0.6b-v3/export_onnx.py
```

Same approach sherpa-onnx team used for 0.6B models.

### File Structure After Conversion

```
/opt/swictation/models/sherpa-onnx-nemo-parakeet-tdt-1.1b-converted/
‚îú‚îÄ‚îÄ encoder.onnx          # ~2.5GB - Acoustic encoder (fp32)
‚îú‚îÄ‚îÄ decoder.onnx          # ~36MB - Prediction network (fp32)
‚îú‚îÄ‚îÄ joiner.onnx           # ~26MB - Joint network (fp32)
‚îú‚îÄ‚îÄ encoder.int8.onnx     # ~640MB - Quantized for CPU
‚îú‚îÄ‚îÄ decoder.int8.onnx     # ~9MB
‚îú‚îÄ‚îÄ joiner.int8.onnx      # ~6.5MB
‚îú‚îÄ‚îÄ tokens.txt            # Vocabulary (9953 tokens)
‚îî‚îÄ‚îÄ encoder.weights       # External weights file
```

### Metadata Required

The conversion adds critical metadata for sherpa-rs:

```python
{
    "vocab_size": 9953,
    "normalize_type": "",
    "pred_rnn_layers": 2,
    "pred_hidden": 640,
    "subsampling_factor": 8,
    "model_type": "EncDecRNNTBPEModel",
    "version": "2",
    "feat_dim": 128,
    "url": "https://huggingface.co/nvidia/parakeet-tdt-1.1b",
}
```

---

## Why 1.1B is Primary Target

**User requirement clarification**: 1.1B model is the **#1 goal**, not 110M.

**Why 1.1B matters**:
- Highest accuracy available for Parakeet-TDT
- Best for medical/legal dictation (critical accuracy needs)
- 1.1B parameters vs 0.6B = ~83% more capacity
- Expected GPU performance: 20-50ms inference (acceptable for real-time)

**Production Strategy**:
- Primary: 1.1B fp32 + GPU (best accuracy + speed)
- Fallback: 0.6B int8 + CPU (when GPU busy, or CPU-only systems)

---

## Dependencies (Isolated in Venv)

```
Python 3.12
nemo-toolkit[asr] 2.5.2
onnx==1.17.0
onnxruntime==1.17.1
numpy<2
torch (with CUDA support)
```

**Why version pins?**: NeMo requires specific ONNX versions. System has conflicting versions for other projects.

---

## Next Steps

1. ‚úÖ ~~Create conversion toolkit~~
2. ‚è≥ Complete NeMo venv installation (~5 min remaining)
3. ‚è≥ Run conversion script (`convert-1.1b.sh`)
4. ‚è≥ Test with sherpa-rs (CPU first, then GPU)
5. ‚è≥ Benchmark GPU performance
6. ‚è≥ Update production strategy docs

**Time to completion**: ~15-30 minutes after venv installation finishes

---

## References

- [Official sherpa-onnx NeMo scripts](https://github.com/k2-fsa/sherpa-onnx/tree/master/scripts/nemo)
- [NeMo Parakeet-TDT 1.1B HuggingFace](https://huggingface.co/nvidia/parakeet-tdt-1.1b)
- [sherpa-onnx NeMo TDT docs](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/nemo-transducer-models.html)

---

**Status**: Ready to proceed with conversion once NeMo installation completes.
